{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(filename):\n",
    "    fo = open(filename, 'rb')\n",
    "    fdict = cPickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return fdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for i in range(1, 6):\n",
    "    data_dict['data_batch_' + str(i)] = unpickle('cucumber_data/p1/data_batch_' + str(i))\n",
    "data_dict['test_batch'] = unpickle('cucumber_data/p1/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images(data, labels):\n",
    "    num_images = data.shape[0] // 3\n",
    "    images = np.zeros((num_images, 32, 32, 3))\n",
    "    labels_arr = np.zeros((num_images, 9))\n",
    "    for i in range(num_images):\n",
    "        images[i] = np.vstack((data[3*i], data[3*i+1], data[3*i+2])).reshape(3, 32, 32).transpose(1,2,0) / 255\n",
    "        labels_arr[i][labels[i]] = 1\n",
    "    return (images, labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_dict = {}\n",
    "labels_dict = {}\n",
    "for data_batch in data_dict:\n",
    "    images_dict[data_batch], labels_dict[data_batch] = get_images(data_dict[data_batch]['data'], data_dict[data_batch]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_images = images_dict['data_batch_5']\n",
    "validation_labels = labels_dict['data_batch_5']\n",
    "\n",
    "test_images = images_dict['test_batch']\n",
    "test_labels = labels_dict['test_batch']\n",
    "\n",
    "train_images = np.vstack((images_dict['data_batch_1'], \n",
    "                          images_dict['data_batch_2'], \n",
    "                          images_dict['data_batch_3'], \n",
    "                          images_dict['data_batch_4']))\n",
    "\n",
    "train_labels = np.vstack((labels_dict['data_batch_1'], \n",
    "                          labels_dict['data_batch_2'], \n",
    "                          labels_dict['data_batch_3'], \n",
    "                          labels_dict['data_batch_4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(images, batch_size):\n",
    "    images_ind = list(range(len(images)))\n",
    "    random.shuffle(images_ind)\n",
    "    return images[images_ind[:batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return tf.maximum(alpha*x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 32*32*3\n",
    "filter_conv1 = 32\n",
    "filter_conv2 = 64\n",
    "#neuron_1 = 4*4*filter_conv2\n",
    "neuron_1 = 8*8*filter_conv2\n",
    "neuron_2 = neuron_1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_examples = train_images.shape[0]\n",
    "iters = 17*num_train_examples\n",
    "batch_size = 200\n",
    "num_subjects = 9\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32, name='images')\n",
    "y_correct = tf.placeholder(shape=[None, num_subjects], dtype=tf.float32, name='correct_output')\n",
    "\n",
    "# first convolution\n",
    "W_conv1 = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, filter_conv1], stddev=0.1), name=\"W_conv1\")\n",
    "b_conv1 = tf.Variable(tf.zeros([filter_conv1]), name=\"b_conv1\")\n",
    "h_conv1 = tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1   # (None, 32, 32, filter_conv1)\n",
    "#h_conv1_relu = tf.nn.relu(h_conv1)\n",
    "h_conv1_relu = leaky_relu(h_conv1)\n",
    "\n",
    "# first max pool\n",
    "h_pool_1 = tf.nn.max_pool(h_conv1_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # (None, 16, 16, filter_conv1)\n",
    "\n",
    "# second convolution\n",
    "W_conv2 = tf.Variable(tf.truncated_normal(shape=[4, 4, filter_conv1, filter_conv2], stddev=0.1), name=\"W_conv2\")\n",
    "b_conv2 = tf.Variable(tf.zeros([filter_conv2]), name=\"b_conv2\")\n",
    "h_conv2 = tf.nn.conv2d(h_pool_1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2   # (None, 16, 16, filter_conv2)\n",
    "#h_conv2_relu = tf.nn.relu(h_conv2)\n",
    "h_conv2_relu = leaky_relu(h_conv2)\n",
    "\n",
    "\n",
    "# second max pool\n",
    "h_pool_2 = tf.nn.max_pool(h_conv2_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # (None, 8, 8, filter_conv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten\n",
    "fc_input = tf.reshape(h_pool_2, [-1, neuron_1])\n",
    "\n",
    "# Fully Connected\n",
    "W_1 = tf.Variable(tf.truncated_normal([neuron_1, neuron_2]), name=\"W_1\")\n",
    "b_1 = tf.Variable(0.0, [neuron_2], name=\"b_1\")\n",
    "a_1 = tf.nn.relu(tf.matmul(fc_input, W_1) + b_1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "a_1_drop = tf.nn.dropout(a_1, keep_prob)\n",
    "\n",
    "W_2 = tf.Variable(tf.truncated_normal([neuron_2, num_subjects]), name=\"W_2\")\n",
    "b_2 = tf.Variable(0.0, [num_subjects], name=\"b_2\")\n",
    "y = tf.matmul(a_1_drop, W_2) + b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_correct, logits=y))\n",
    "\n",
    "# Optimization\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Evaluate model\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_correct,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\n",
      "Accuracy validation: 0.10303031653165817\n",
      "step 1000, training accuracy 0.455\n",
      "Accuracy validation: 0.6161617040634155\n",
      "step 2000, training accuracy 0.635\n",
      "Accuracy validation: 0.6848485469818115\n",
      "step 3000, training accuracy 0.715\n",
      "Accuracy validation: 0.7171717882156372\n",
      "step 4000, training accuracy 0.775\n",
      "Accuracy validation: 0.7353535890579224\n",
      "step 5000, training accuracy 0.795\n",
      "Accuracy validation: 0.7333333492279053\n",
      "step 6000, training accuracy 0.835\n",
      "Accuracy validation: 0.7575757503509521\n",
      "step 7000, training accuracy 0.875\n",
      "Accuracy validation: 0.7777777910232544\n",
      "step 8000, training accuracy 0.865\n",
      "Accuracy validation: 0.8141414523124695\n",
      "step 9000, training accuracy 0.885\n",
      "Accuracy validation: 0.8161616921424866\n",
      "step 10000, training accuracy 0.91\n",
      "Accuracy validation: 0.8323233127593994\n",
      "step 11000, training accuracy 0.905\n",
      "Accuracy validation: 0.8161616325378418\n",
      "step 12000, training accuracy 0.94\n",
      "Accuracy validation: 0.8242424726486206\n",
      "step 13000, training accuracy 0.94\n",
      "Accuracy validation: 0.8444444537162781\n",
      "step 14000, training accuracy 0.955\n",
      "Accuracy validation: 0.8424242734909058\n",
      "step 15000, training accuracy 0.97\n",
      "Accuracy validation: 0.8363636136054993\n",
      "step 16000, training accuracy 0.975\n",
      "Accuracy validation: 0.8444443941116333\n",
      "step 17000, training accuracy 0.98\n",
      "Accuracy validation: 0.8303030133247375\n",
      "step 18000, training accuracy 0.965\n",
      "Accuracy validation: 0.8646464347839355\n",
      "step 19000, training accuracy 0.965\n",
      "Accuracy validation: 0.8404040932655334\n",
      "step 20000, training accuracy 0.975\n",
      "Accuracy validation: 0.8505051136016846\n",
      "step 21000, training accuracy 0.995\n",
      "Accuracy validation: 0.8383837938308716\n",
      "step 22000, training accuracy 0.985\n",
      "Accuracy validation: 0.8545454740524292\n",
      "step 23000, training accuracy 0.995\n",
      "Accuracy validation: 0.8585858345031738\n",
      "step 24000, training accuracy 0.98\n",
      "Accuracy validation: 0.8545454144477844\n",
      "step 25000, training accuracy 0.99\n",
      "Accuracy validation: 0.8646464943885803\n",
      "step 26000, training accuracy 0.995\n",
      "Accuracy validation: 0.8505050539970398\n",
      "step 27000, training accuracy 1\n",
      "Accuracy validation: 0.8626262545585632\n",
      "step 28000, training accuracy 1\n",
      "Accuracy validation: 0.8626262545585632\n",
      "step 29000, training accuracy 1\n",
      "Accuracy validation: 0.8787878751754761\n",
      "step 30000, training accuracy 1\n",
      "Accuracy validation: 0.8525252938270569\n",
      "step 31000, training accuracy 1\n",
      "Accuracy validation: 0.8767677545547485\n",
      "step 32000, training accuracy 0.995\n",
      "Accuracy validation: 0.8565656542778015\n",
      "step 33000, training accuracy 1\n",
      "Accuracy validation: 0.8525252938270569\n",
      "DONE. Test accuracy: 0.8484848141670227\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "validation_feed_dict = {x: validation_images, y_correct: validation_labels, keep_prob: 1.0 }\n",
    "test_feed_dict = {x: test_images, y_correct: test_labels, keep_prob: 1.0}\n",
    "for i in range(iters):\n",
    "    train_indices = random.sample(range(num_train_examples), batch_size)\n",
    "    train_feed_dict = {x: train_images[train_indices], y_correct: train_labels[train_indices], keep_prob:0.4 }\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict=train_feed_dict)\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"Accuracy validation: {}\".format(accuracy.eval(\n",
    "                feed_dict = validation_feed_dict)))\n",
    "    _ = sess.run(train_step, feed_dict = train_feed_dict)\n",
    "\n",
    "print(\"DONE. Test accuracy: {}\".format(accuracy.eval(feed_dict = test_feed_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
