{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(filename):\n",
    "    fo = open(filename, 'rb')\n",
    "    fdict = cPickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return fdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for i in range(1, 6):\n",
    "    data_dict['data_batch_' + str(i)] = unpickle('cucumber_data/p1/data_batch_' + str(i))\n",
    "data_dict['test_batch'] = unpickle('cucumber_data/p1/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images(data, labels):\n",
    "    num_images = data.shape[0] // 3\n",
    "    images = np.zeros((num_images, 32, 32, 3))\n",
    "    labels_arr = np.zeros((num_images, 9))\n",
    "    for i in range(num_images):\n",
    "        images[i] = np.vstack((data[3*i], data[3*i+1], data[3*i+2])).reshape(3, 32, 32).transpose(1,2,0) / 255\n",
    "        labels_arr[i][labels[i]] = 1\n",
    "    return (images, labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_dict = {}\n",
    "labels_dict = {}\n",
    "for data_batch in data_dict:\n",
    "    images_dict[data_batch], labels_dict[data_batch] = get_images(data_dict[data_batch]['data'], data_dict[data_batch]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = np.vstack((images_dict['data_batch_1'], \n",
    "                          images_dict['data_batch_2'], \n",
    "                          images_dict['data_batch_3'], \n",
    "                          images_dict['data_batch_4']))\n",
    "\n",
    "train_labels = np.vstack((labels_dict['data_batch_1'], \n",
    "                          labels_dict['data_batch_2'], \n",
    "                          labels_dict['data_batch_3'], \n",
    "                          labels_dict['data_batch_4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_train = np.mean(train_images,axis=0)\n",
    "mean_train = np.mean(train_images)\n",
    "\n",
    "train_images -= mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920, 32, 32, 3)\n",
      "(7920, 9)\n"
     ]
    }
   ],
   "source": [
    "def jiggling_image(image):\n",
    "    #noise = np.random.rand(image.shape[0], image.shape[1], image.shape[2])\n",
    "    noise = np.random.normal(0, 0.01, image.shape)\n",
    "    new_image = image + noise\n",
    "    new_image = np.minimum(1, new_image)\n",
    "    new_image = np.maximum(0, new_image)\n",
    "    return new_image\n",
    "\n",
    "def jiggling_images(image, label, new_num=3):\n",
    "    new_images = np.zeros((new_num, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    #new_labels = np.ones((new_num, 1)) * int(label)\n",
    "    new_labels = np.tile(label, new_num).reshape((new_num, 9))\n",
    "    for i in range(new_num):\n",
    "        new_images[i] = jiggling_image(image)\n",
    "    return (new_images, new_labels)\n",
    "        \n",
    "\n",
    "len_train_images = len(train_images)\n",
    "\n",
    "for i in range(len_train_images):\n",
    "    new_images, new_labels = jiggling_images(train_images[i], train_labels[i])\n",
    "    train_images = np.vstack((train_images, new_images))\n",
    "    train_labels = np.vstack((train_labels, new_labels))\n",
    "print(train_images.shape)\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_images = images_dict['data_batch_5'] - mean_train\n",
    "validation_labels = labels_dict['data_batch_5']\n",
    "\n",
    "test_images = images_dict['test_batch'] - mean_train\n",
    "test_labels = labels_dict['test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(images, batch_size):\n",
    "    images_ind = list(range(len(images)))\n",
    "    random.shuffle(images_ind)\n",
    "    return images[images_ind[:batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return tf.maximum(alpha*x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "input_size = 32*32*3\n",
    "filter_conv1 = 12\n",
    "filter_conv2 = 20\n",
    "#neuron_1 = 4*4*filter_conv2\n",
    "neuron_1 = 8*8*filter_conv2\n",
    "neuron_2 = neuron_1 // 4\n",
    "W_stddev = 1\n",
    "beta = 0.01\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_train_examples = train_images.shape[0]\n",
    "sqrt_num_train_examples = math.sqrt(num_train_examples)\n",
    "iters = 2*num_train_examples\n",
    "batch_size = 250\n",
    "num_subjects = 9\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32, name='images')\n",
    "y_correct = tf.placeholder(shape=[None, num_subjects], dtype=tf.float32, name='correct_output')\n",
    "\n",
    "# first convolution\n",
    "W_conv1 = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, filter_conv1], stddev=0.1), name=\"W_conv1\")\n",
    "b_conv1 = tf.Variable(tf.zeros([filter_conv1]), name=\"b_conv1\")\n",
    "h_conv1 = tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1   # (None, 32, 32, filter_conv1)\n",
    "#h_conv1_relu = tf.nn.relu(h_conv1)\n",
    "h_conv1_relu = leaky_relu(h_conv1)\n",
    "\n",
    "# first max pool\n",
    "h_pool_1 = tf.nn.max_pool(h_conv1_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # (None, 16, 16, filter_conv1)\n",
    "\n",
    "# second convolution\n",
    "W_conv2 = tf.Variable(tf.truncated_normal(shape=[3, 3, filter_conv1, filter_conv2], stddev=0.1), name=\"W_conv2\")\n",
    "b_conv2 = tf.Variable(tf.zeros([filter_conv2]), name=\"b_conv2\")\n",
    "h_conv2 = tf.nn.conv2d(h_pool_1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2   # (None, 16, 16, filter_conv2)\n",
    "#h_conv2_relu = tf.nn.relu(h_conv2)\n",
    "h_conv2_relu = leaky_relu(h_conv2)\n",
    "\n",
    "\n",
    "# second max pool\n",
    "h_pool_2 = tf.nn.max_pool(h_conv2_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # (None, 8, 8, filter_conv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten\n",
    "fc_input = tf.reshape(h_pool_2, [-1, neuron_1])\n",
    "\n",
    "# Fully Connected\n",
    "W_1 = tf.Variable(tf.truncated_normal([neuron_1, neuron_2], stddev=W_stddev) / sqrt_num_train_examples, name=\"W_1\")\n",
    "b_1 = tf.Variable(0.0, [neuron_2], name=\"b_1\")\n",
    "a_1 = tf.nn.relu(tf.matmul(fc_input, W_1) + b_1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "a_1_drop = tf.nn.dropout(a_1, keep_prob)\n",
    "\n",
    "W_2 = tf.Variable(tf.truncated_normal([neuron_2, num_subjects], stddev=W_stddev), name=\"W_2\")\n",
    "b_2 = tf.Variable(0.0, [num_subjects], name=\"b_2\")\n",
    "y = tf.matmul(a_1_drop, W_2) + b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularization_loss = beta * (tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(W_conv2) + tf.nn.l2_loss(W_1) + tf.nn.l2_loss(W_2))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_correct, logits=y) + regularization_loss)\n",
    "\n",
    "# Optimization\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Evaluate model\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_correct,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.108\n",
      "Accuracy validation: 0.10909092426300049\n",
      "step 1000, training accuracy 0.748\n",
      "Accuracy validation: 0.7717171907424927\n",
      "step 2000, training accuracy 0.788\n",
      "Accuracy validation: 0.7959596514701843\n",
      "step 3000, training accuracy 0.816\n",
      "Accuracy validation: 0.8101010322570801\n",
      "step 4000, training accuracy 0.836\n",
      "Accuracy validation: 0.8161616325378418\n",
      "step 5000, training accuracy 0.84\n",
      "Accuracy validation: 0.802020251750946\n",
      "step 6000, training accuracy 0.86\n",
      "Accuracy validation: 0.7999999523162842\n",
      "step 7000, training accuracy 0.872\n",
      "Accuracy validation: 0.8181818723678589\n",
      "step 8000, training accuracy 0.856\n",
      "Accuracy validation: 0.820202112197876\n",
      "step 9000, training accuracy 0.872\n",
      "Accuracy validation: 0.8161616325378418\n",
      "step 10000, training accuracy 0.9\n",
      "Accuracy validation: 0.8262625932693481\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "validation_feed_dict = {x: validation_images, y_correct: validation_labels, keep_prob: 1.0 }\n",
    "test_feed_dict = {x: test_images, y_correct: test_labels, keep_prob: 1.0}\n",
    "for i in range(iters):\n",
    "    train_indices = random.sample(range(num_train_examples), batch_size)\n",
    "    train_feed_dict = {x: train_images[train_indices], y_correct: train_labels[train_indices], keep_prob:0.5 }\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict=train_feed_dict)\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"Accuracy validation: {}\".format(accuracy.eval(feed_dict = validation_feed_dict)))\n",
    "    _ = sess.run(train_step, feed_dict = train_feed_dict)\n",
    "\n",
    "print(\"DONE. Test accuracy: {}\".format(accuracy.eval(feed_dict = test_feed_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
