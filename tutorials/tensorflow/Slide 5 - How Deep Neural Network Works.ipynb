{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài giảng về học Deep Neural Network \n",
    "## MaSSP 2017, Computer Science\n",
    "### Chuẩn bị: Nguyễn Vương Linh, MIT Class of 2017\n",
    "\n",
    "Các bài giảng trước (Slide 3 & 4) đã đề cập đến Deep Neural Network (DNN) và ứng dụng trong một số bài toán Machine Learning cơ bản. Trong bài giảng này, chúng ta tập trung vào vấn đề _học_ DNN: làm sao để xác định được giá trị các trọng số có trong DNN. Chúng ta giả sử cấu trúc mạng DNN được biết trước: có bao nhiêu lớp trong DNN và mỗi lớp có bao nhiêu neurons. Slide 6 & 7 sẽ hướng dẫn các bạn cụ thể hơn làm sao để chọn được mạng DNN phù hợp trong từng bài toán cụ thể.\n",
    "\n",
    "Một số kí hiệu được sử dụng trong bài giảng:\n",
    "1. D: mạng DNN.\n",
    "2. L: số lớp có trong D, đánh số từ 0. Cụ thể hơn, lớp thứ 0 tương ứng với dữ liệu và lớp thứ L tương ứng với nhãn.\n",
    "3. $m_i$ ($i = 0, 1, .., L$): số neuron có trong lớp thứ $i$. Đôi khi sẽ sử dụng $m = (m_0, m_1, .., m_L)$ để kí hiệu số neuron có trong từng lớp.\n",
    "4. $w^{(i)}_{jk}$: trọng số của cạnh nối neuron thứ $j$ trong lớp $i$ đến neuron thứ $k$ trong lớp $i + 1$, và $b^i_j$ là thiên lệch của neuron thứ $j$ trong lớp $i$.\n",
    "\n",
    "__Bài tập__: sử dụng các kí hiệu mô tả như trên, hãy vẽ một DNN với $L = 3, m = (5, 3, 3, 2)$ và trọng số $w$ tuỳ ý."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Giải thích học DNN trong 10 dòng\n",
    "Cho đến giờ, trong tay bạn là $n$ bộ dữ liệu $x_1, x_2, .., x_n$ và nhãn tương ứng là $y_1, y_2, .., y_n$. Làm sao để xác định được trọng số $w$? Có lẽ nhiều bạn sẽ hình dung được cách làm nói chung là như sau:\n",
    "\n",
    "1. Thử từng bộ $(x_i, y_i)$.\n",
    "2. Thuật toán feedforward sử dụng giá trị $x_i$ để tính kết quả $D(x_i)$ ở lớp cuối cùng.\n",
    "3. So sánh $D(x_i)$ với $y_i$.\n",
    "4. Sử dụng sai số để điều chỉnh lại trọng số $w^{(i)}_{jk}$ một cách hợp lý.\n",
    "5. Lặp lại bước 1 đến khi bạn cảm thấy ưng ý!\n",
    "\n",
    "Hiển nhiên triển khai ý tưởng nói trên một cách chi tiết đòi hỏi nhiều công sức. Slide 4 đã đề cập bước 2, do đó trong slide này mình sẽ đi sâu vào bước 3, 4 và 5, cũng như đề cập những vấn đề phát sinh khi học DNN. Trong các tài liệu chuyên sâu về DNN, thuật toán nói trên được biết đến với tên _truyền ngược sai số_ (backpropagation), với ý tưởng sử dụng sai số giữa $D(x_i)$ và $y_i$ để điều chỉnh lại các trọng số $w$ dùng để tính toán $D(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Giải thích chi tiết\n",
    "## 1.1. Sử dụng sai số nào?\n",
    "Như đã đề cập ở Slide 3 & 4, với dữ liệu $x_i$, $D(x_i)$ không trực tiếp trả lại kết quả (nhãn nào được chọn), mà trả ra một vector biểu diễn _phân phối xác suất p_, với $p_j$ là xác suất chọn nhãn thứ $j$, và nhãn của $D(x_i)$ là phần tử $k$ với giá trị $p_k$ lớn nhất. \n",
    "\n",
    "Tuy nhiên, không phải phân phối nào cũng giống nhau, kể cả trong số những phân phối cho ra cùng kết quả. Giả sử $(1, 0, 0)$ là vector biểu diễn nhãn thứ nhất. Rõ ràng bạn sẽ tin tưởng phân phối $(0.9, 0.05, 0.05)$ hơn là phân phối $(0.5, 0.25, 0.25)$. Kể cả nếu như phân phối cho ra kết quả sai, bạn vẫn tin tưởng phân phối $(0.3, 0.3, 0.4)$ hơn là phân phối $(0, 1, 0)$.\n",
    "\n",
    "Trong toán học, bạn có thể mô tả khái niệm nói trên bằng _khoảng cách_ giữa hai phân phối, lấy ý tưởng dựa trên khoảng cách giữa hai điểm trên hệ trục toạ độ. Trong không gian Descartes, khoảng cách giữa 2 điểm $(r_1, s_1)$ và $(r_2, s_2)$ là $$\\sqrt{(r_1 - r_2)^2 + (s_1 - s_2)^2}$$\n",
    "\n",
    "Trong Slide này, chúng ta sẽ sử dụng _bình phương_ khoảng cách giữa phân phối $D(x_i)$ và $y_i$, kí hiệu bằng $\\delta(D(x_i), y_i$): $$\\delta(D(x_i), y_i) = ||D(x_i) - y_i||_2^2 = \\sum_{j = 1}^m |D(x_i)_j - y_{ij}|^2$$\n",
    "\n",
    "Có hai lí do để sử dụng bình phương khoảng cách, thay cho tổng khoảng cách $\\sum_{j = 1}^m |D(x_i)_j - y_{ij}|$:\n",
    "* Bạn muốn đánh lỗi nặng hơn những giá trị xa giá trị thực hơn.\n",
    "* Bình phương khoảng cách giúp việc tính đạo hàm trở nên đơn giản.\n",
    "\n",
    "Lưu ý rằng bình phương khoảng cách chỉ là một trong rất nhiều cách tính sai số. Trong Lab 3 & 4 cách tính sai số cross-entropy sẽ được sử dụng. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Điều chỉnh trọng số\n",
    "Với mỗi bộ $(x, y)$, chúng ta tính toán:\n",
    "\n",
    "* Giá trị nhận được ở lớp cuối cùng $a^L$. Lưu ý là chúng ta khởi tạo $a^0 = x$, $z^i = w^{i - 1} a^{i - 1} + b^i$ và $a^i = \\sigma(z^i)$ với $i = 0, 1, .., L$.\n",
    "\n",
    "* Đặt sai số $C = \\frac{1}{2n}||y - a^L(x)||$ và tính toán sai số ở lớp cuối cùng: $$\\delta^L = \\nabla_a C \\odot \\sigma'(z^L)$$.\n",
    "\n",
    "* Truyền ngược sai số: tính ngược với $l = L - 1, L - 2, ..$, đặt $\\delta^l = ((w^l)^T \\delta^{l + 1}) \\odot \\sigma'(z^l)$.\n",
    "\n",
    "* Giá trị gradient được tính theo công thức $$\\frac{\\partial C}{\\partial w^l_{jk}} = a_k^l \\delta^l_j$$ và $$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$. \n",
    "\n",
    "* Điều chỉnh trọng số $w$ và thiên lệch $b$ tương ứng, ví dụ: $$b^i_j = b^i_j - \\alpha \\frac{\\partial C}{\\partial b^l_j}$$\n",
    "\n",
    "Hằng số $\\alpha > 0$ là _tốc độ học_, thể hiện sự thay đổi hệ số ứng với thông tin mới nhận được. Nếu chọn $\\alpha$ quá nhỏ, hệ số trong mạng DNN sẽ không tiếp nhận thông tin mới. Nếu chọn $\\alpha$ quá lớn, mạng DNN sẽ bị _overfit_ (điều chỉnh để khớp với dữ liệu cho trước). Một trong những cách tránh cả hai vấn đề này là thiết lập $\\alpha$ lớn lúc ban đầu, và giảm dần $\\alpha$ khi lượng dữ liệu tăng thêm.\n",
    "\n",
    "Đoạn chương trình sau đây (trích từ http://neuralnetworksanddeeplearning.com/chap2.html) minh hoạ thuật toán truyền ngược (lưu ý chương trình này chỉ mang tính minh hoạ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
    "        gradient for the cost function C_x.  \"nabla_b\" and\n",
    "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
    "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y) \n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tóm tắt các phương trình trong truyền ngược\n",
    "\n",
    "$\\delta^L = \\nabla_a C \\odot \\sigma'(z^L)$\n",
    "\n",
    "$\\delta^l = ((w^L)^T \\delta^{l + 1}) \\odot'(z^l)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^l_{jk}} = a^l_k \\delta^l_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
