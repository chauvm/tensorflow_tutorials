{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Neural Network\n",
    "## MaSSP 2017, Computer Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Trong lab này, chúng ta sẽ xây dựng những neural network đơn giản để phân loại các chữ số viết tay trong kho dữ liệu MNIST.\n",
    "\n",
    "__Phần 1__: code cho sẵn của một neural network chỉ có input layer và output layer. Ta sẽ sử dụng _softmax regression_ cho output layer, và hàm _cross-entropy_ khi tính cost function. Với cách chọn hàm như vậy, network này hoàn toàn tương đồng với thuật toán _logistic regression_ cho nhiều hơn 2 nhóm trong lab trước."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Phần 2__: hướng dẫn thêm vào neural network ở phần 1 một hidden layer.\n",
    "\n",
    "__Phần 3__: bài tập thêm một hidden layer nữa vào network ở phần 2, thu được một _deep neural network_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Network với input và output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "(Dựa theo https://www.tensorflow.org/get_started/mnist/beginners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Khái quát\n",
    "\n",
    "<img style=\"float: left\" src=\"../../images/graphs/neural_network_0_hidden_layer.png\"/>\n",
    "Trong phần này, chúng ta sẽ xây dựng một network không có hidden layer mà chỉ có input layer gồm 784 neuron được kết nối thẳng đến output layer gồm 10 neuron tương ứng với 10 nhóm chữ số viết tay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kết quả __logits__ của 10 neuron này sẽ được biến đổi ở bước softmax regression để cuối cùng ta thu được 10 giá trị xác suất (là các số thực dương và tổng là 1) ứng với xác suất mỗi hình ảnh thuộc về từng nhóm chữ số.\n",
    "\n",
    "Sai số giữa label dự đoán và label thật được tính bằng hàm cross-entropy.\n",
    "\n",
    "Một lần nữa chúng ta sẽ dùng $GradientDescentOptimizer$ để thay đổi $weight$ và $bias$ giữa input layer và output layer, sao cho sai số này là nhỏ nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kết thúc quá trình học, chúng ta sẽ kiểm tra độ chính xác của model với __test set__, bằng cách áp dụng bước _feed forward_ như trong quá trình học để thu được 10 giá trị xác suất cho mỗi hình ảnh trong test set.\n",
    "\n",
    "Sau đó, mỗi hình ảnh được gán cho nhóm chữ số mà giá trị xác suất hình ảnh này ứng với nhóm đó là cao nhất trong 10 giá trị.\n",
    "\n",
    "Độ chính xác của model sẽ được tính bằng phần trăm số hình ảnh được phân loại đúng trong test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Xây dựng network\n",
    "Network đơn giản này được xây dựng tương tự như Logistic Regression cho 10 nhóm trong lab trước. \n",
    "\n",
    "__Checkpoint 1__: Hãy đọc toàn bộ đoạn code này và hỏi mentor nếu có vướng mắc. Ngoài ra, toàn bộ code có thể được tìm thấy trong file \"Deep_Neural_Network_part_1.py\". Chú ý cách sử dụng $Dataset.next\\_batch()$ để lấy một nhóm nhỏ data đưa vào $train\\_step$. Theo bạn tại sao chúng ta làm như vậy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "with tf.name_scope(\"Input\") as scope:\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")               \n",
    "    y_correct = tf.placeholder(tf.float32, [None, 10], name=\"y-correct_label\")\n",
    "    \n",
    "with tf.name_scope(\"Weight\") as scope:\n",
    "    W = tf.Variable(tf.zeros([784, 10]), name=\"weight\")                       \n",
    "    \n",
    "with tf.name_scope(\"Bias\") as scope:\n",
    "    b = tf.Variable(tf.zeros(10), name=\"bias\")                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Softmax\") as scope:\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b, name=\"softmax\")                # * \n",
    "    \n",
    "with tf.name_scope(\"Cross_Entropy\") as scope:                             # *\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_correct * tf.log(y), \n",
    "                                                  reduction_indices=[1]), \n",
    "                                   name=\"cross_entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\*: 2 bước này được viết tách biệt để thấy rõ trình tự các bước. Khi viết code bằng TensorFlow nên sử dụng $tf.nn.softmax\\_cross\\_entropy\\_with\\_logits$ trực tiếp lên $tf.matmul(x, W) + b$. https://www.tensorflow.org/get_started/mnist/beginners#training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Train') as scope:\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_correct,1), name = \"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Launch the model\n",
    "sess = tf.InteractiveSession()\n",
    "file_writer = tf.summary.FileWriter(\"DNN_0_hidden_layer\", sess.graph)\n",
    "# create a summary for our cost and accuracy\n",
    "tf.summary.scalar(\"cost_summary\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "# merge all summaries into a single operation \n",
    "# which we can execute in a session \n",
    "summary_step = tf.summary.merge_all()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)    # batch_size = 100\n",
    "    _, summary = sess.run([train_step, summary_step], \n",
    "                          feed_dict={x: batch_xs, y_correct: batch_ys})\n",
    "    # logging\n",
    "    file_writer.add_summary(summary, i)\n",
    "# Test\n",
    "print(\"Accuracy: {}\".format(accuracy.eval(feed_dict={x: mnist.test.images, \n",
    "                                                     y_correct: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Network với 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1 Khái quát\n",
    "<img style=\"float: left\" src=\"../../images/graphs/neural_network_1_hidden_layer.png\"/>\n",
    "\n",
    "Chúng ta sẽ thêm vào network đơn giản ở trên 1 hidden layer nữa. Đầu tiên hãy chọn số neuron cho lớp ẩn này - một số nguyên bất kì $neuron\\_1$, chúng ta sẽ thay đổi số nguyên này sau khi xây dựng model hoàn chỉnh.\n",
    "\n",
    "$neuron\\_1$ = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Input layer đến hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Inputs\n",
    "__Checkpoint 2__: Dựa vào giá trị đã chọn của $neuron\\_1$, hãy mô tả kích thước của các tensor sau:\n",
    "* input $X$\n",
    "* weight $W\\_1$\n",
    "* bias $b\\_1$\n",
    "* weighted sum $z$\n",
    "* hidden layer result $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Activation function\n",
    "Chọn một trong các activation function sau đây: linear, sigmoid ($tf.sigmoid$), hyperbolic tangent, ReLU ($tf.nn.relu$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hidden layer đến output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Inputs\n",
    "__Checkpoint 3__: Dựa vào giá trị đã chọn của $neuron\\_1$, hãy mô tả kích thước của các tensor sau:\n",
    "* hidden layer result $a$\n",
    "* weight $W\\_2$\n",
    "* bias $b\\_2$\n",
    "* weighted sum $z$\n",
    "* output $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Activation function\n",
    "Ta sẽ sử dụng _softmax regression_ cho lớp output thường được sử dụng. \n",
    "\n",
    "<i>Lưu ý cách dùng $tf.nn.softmax\\_cross\\_entropy\\_with\\_logits$ trong TensorFlow.</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cost/Loss Function, Optimizer\n",
    "\n",
    "* Công thức hàm cross-entropy, một loại cost function\n",
    "\n",
    "$$cost = H_{y'}(y) = -\\sum_i y'_i \\log(y_i)$$\n",
    "\n",
    "* TensorFlow cung cấp sẵn một số optimizers, $GradientDescentOptimizer$ chỉ là một trong số đó: https://www.tensorflow.org/api_guides/python/train#Optimizers.\n",
    "\n",
    "Chọn một trong các optimizers sau: GradientDescentOptimizer, AdamOptimizer, AdadeltaOptimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Checkpoint 4__: Trước khi tiến hành xây dựng network, hãy vẽ ra giấy sơ đồ của network và ghi rõ các lựa chọn thông số, hàm activation function, optimizer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2 Xây dựng network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load data\n",
    "Đầu tiên, import các thư viện cần thiết và dữ liệu MNIST như các lab trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import tensorflow, matplotlib, load MNIST dataset (one_hot=True)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Các tham số trong quá trình học\n",
    "Hãy điền các tham số cần thiết\n",
    "* $neuron\\_1$: số neuron trong lớp ẩn đã chọn ở trên\n",
    "* $num\\_steps$: số lần gọi train_step\n",
    "* $batch\\_size$: số lượng ảnh sẽ dùng trong một batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "neuron_1 = \n",
    "num_steps = \n",
    "batch_size = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input\n",
    "Khởi tạo tensor $x$ và $y\\_correct$, chứa ảnh và label đúng của dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Input\") as scope:\n",
    "    x = \n",
    "    y_correct ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input layer -> Hidden layer\n",
    "Khởi tạo weight $W\\_1$, bias $b\\_1$, và tính weighted sum $z\\_1$, hidden layer output $a$. \n",
    "\n",
    "Lưu ý khi khởi tạo weight và bias, ta có thể dùng $tf.random\\_normal(shape)$ thay vì $tf.zeros(shape)$ để các giá trị ban đầu của các tham số là khác nhau. Xem thêm https://www.tensorflow.org/api_docs/python/tf/random_normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Input_to_Hidden_layer\") as scope:\n",
    "    W_1 = \n",
    "    b_1 = \n",
    "    z_1 = \n",
    "    a = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden layer -> Output layer\n",
    "Khởi tạo weight $W\\_2$, bias $b\\_2$, và tính $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Hidden_Layer_to_Output\") as scope:\n",
    "    W_2 = \n",
    "    b_2 = \n",
    "    y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization\n",
    "Hãy định nghĩa hàm sai số cost/loss function dựa trên label thật $y\\_correct$ và label dự đoán $y$. Chú ý xem $y$ đã được normalized hay không ở bước trên để chọn cách tính phù hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Cost_function\") as scope:\n",
    "    cost ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chọn một optimizer để điều chỉnh thông số trong network sao cho $cost$ ở trên giảm đi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Train') as scope:\n",
    "    train_step ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cuối cùng, định nghĩa hàm tính độ chính xác của thuật toán dự trên label thật $y\\_correct$ và label dự đoán $y$ (của bất kì dataset nào, training/validation/test), tương tự như trong lab trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_correct,1), name = \"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Checkpoint 5__: Trước khi thực hiện quá trình học, hãy copy tất cả đoạn code trên vào một file python \".py\" và đưa mentor xem qua (:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Launch\n",
    "Khởi tạo session và chạy thuật toán.\n",
    "\n",
    "Dưới đây là __một ví dụ__ cách chạy thuật toán và log kết quả biểu diễn trên TensorBoard. Bạn hãy thử các cách chạy thuật toán khác như đã gợi ý ở checkpoint 1 để thu được giá trị phân loại tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Launch the model\n",
    "sess = tf.InteractiveSession()\n",
    "file_writer = tf.summary.FileWriter(\"DNN_1_hidden_layer_graphs\", sess.graph)\n",
    "tf.summary.scalar(\"cost_summary\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_step = tf.summary.merge_all()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    _, summary = sess.run([train_step, summary_step], \n",
    "                          feed_dict={x: batch_xs, y_correct: batch_ys})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Accuracy validation: {}\".format(accuracy.eval(feed_dict = {\n",
    "                                        x: mnist.validation.images, \n",
    "                                        y_correct: mnist.validation.labels})))\n",
    "    # logging\n",
    "    file_writer.add_summary(summary, i)\n",
    "# Test\n",
    "print(\"Accuracy: {}\".format(accuracy.eval(feed_dict = {\n",
    "                     x: mnist.test.images, y_correct: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b>Bài tập:</b> Hãy thay đổi giá trị của số lần lặp, tốc độ học, optimizer, cost function, activation function... và ghi chép lại kết quả thu được. Các số liệu này sẽ giúp ích cho các lab sau và final project. Hãy plot một trong những ảnh mà network này không phân loại đúng, one-hot vector mà network dự đoán cho ảnh này là gì?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Network với nhiều hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b>Bài tập:</b> Hãy thêm hidden layers vào network ở trên, và theo dõi tốc độ học và độ chính xác của network mới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
